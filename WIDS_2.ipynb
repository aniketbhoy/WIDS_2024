{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iX3vbBIcNMPK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c926fcfd-76a9-4cca-d81d-98eb55e6bf17"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1: Training the linear regression model...\n",
            "Trained weights (should be close to [3, 2]): [3.12764058 1.95288136]\n",
            "Step 2: Making predictions...\n",
            "Sample Predictions: [3.12764058 3.52618779 3.92473501 4.32328222 4.72182944]\n",
            "Step 3: Calculating Mean Squared Error...\n",
            "Mean Squared Error (MSE): 0.46836737184689176\n",
            "\n",
            "Testing individual functions:\n",
            "Test MSE (expected ~0.01): 0.010000000000000018\n",
            "Test Gradient (expected ~[-2, -6.67]): [-2.         -4.66666667]\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Linear Regression Assignment with Bias Term\n",
        "\n",
        "Complete the following functions to implement linear regression.\n",
        "Each function will be tested in the main function.\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def mean_squared_error(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Calculate Mean Squared Error (MSE) between true and predicted values.\n",
        "\n",
        "    Parameters:\n",
        "    y_true (numpy array): True target values\n",
        "    y_pred (numpy array): Predicted target values\n",
        "\n",
        "    Returns:\n",
        "    float: MSE value\n",
        "    \"\"\"\n",
        "    # TODO: Implement the formula for MSE\n",
        "    mse = 0.5 * np.mean((y_true - y_pred)**2)\n",
        "    return mse\n",
        "\n",
        "\n",
        "def compute_gradient(X, y, weights):\n",
        "    \"\"\"\n",
        "    Compute the gradient for linear regression.\n",
        "\n",
        "    Parameters:\n",
        "    X (numpy array): Feature matrix (n_samples x n_features)\n",
        "    y (numpy array): Target values\n",
        "    weights (numpy array): Model weights\n",
        "\n",
        "    Returns:\n",
        "    numpy array: Gradient vector\n",
        "    \"\"\"\n",
        "    m = X.shape[0]\n",
        "    predictions = np.dot(X,weights)\n",
        "    gradient = (1/m) * np.dot(X.T,predictions - y)\n",
        "    return gradient\n",
        "\n",
        "\n",
        "def train_linear_regression(X, y, learning_rate, epochs):\n",
        "    \"\"\"\n",
        "    Train a linear regression model using gradient descent.\n",
        "\n",
        "    Parameters:\n",
        "    X (numpy array): Feature matrix (n_samples x n_features)\n",
        "    y (numpy array): Target values\n",
        "    learning_rate (float): Learning rate for gradient descent\n",
        "    epochs (int): Number of iterations\n",
        "\n",
        "    Returns:\n",
        "    numpy array: Final weights\n",
        "    \"\"\"\n",
        "    # TODO: Initialize weights, iterate over epochs, and update weights\n",
        "    weights = np.zeros(X.shape[1])\n",
        "    for epoch in range(epochs):\n",
        "        gradient = compute_gradient(X, y, weights)\n",
        "        weights -= learning_rate * gradient\n",
        "\n",
        "    return weights\n",
        "\n",
        "\n",
        "def predict(X, weights):\n",
        "    \"\"\"\n",
        "    Predict the target values using the learned weights.\n",
        "\n",
        "    Parameters:\n",
        "    X (numpy array): Feature matrix (n_samples x n_features)\n",
        "    weights (numpy array): Model weights\n",
        "\n",
        "    Returns:\n",
        "    numpy array: Predicted target values\n",
        "    \"\"\"\n",
        "    # TODO: Implement the prediction logic\n",
        "    return np.dot(X, weights)\n",
        "\n",
        "\n",
        "def generate_data(m, c, num_samples, noise_level):\n",
        "    \"\"\"\n",
        "    Generate data based on a standard line y = mx + c with noise.\n",
        "\n",
        "    Parameters:\n",
        "    m (float): Slope of the line\n",
        "    c (float): Intercept of the line\n",
        "    num_samples (int): Number of data points to generate\n",
        "    noise_level (float): Standard deviation of noise to add\n",
        "\n",
        "    Returns:\n",
        "    tuple: Feature matrix X and target vector y\n",
        "    \"\"\"\n",
        "    X = np.linspace(0, 10, num_samples).reshape(-1, 1)\n",
        "    noise = np.random.normal(0, noise_level, num_samples)\n",
        "    y = m * X.squeeze() + c + noise\n",
        "    return X, y\n",
        "\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Main function to test all components of the assignment.\n",
        "    \"\"\"\n",
        "    # Generate data for y = 2x + 3 with some noise\n",
        "    m, c = 2, 3\n",
        "    num_samples = 50\n",
        "    noise_level = 1.0\n",
        "    X, y = generate_data(m, c, num_samples, noise_level)\n",
        "\n",
        "    # Append a column of ones to X for the bias term\n",
        "    X_with_bias = np.hstack((np.ones((X.shape[0], 1)), X))\n",
        "\n",
        "    # Parameters for training\n",
        "    learning_rate = 0.01\n",
        "    epochs = 1000\n",
        "\n",
        "    print(\"Step 1: Training the linear regression model...\")\n",
        "    weights = train_linear_regression(X_with_bias, y, learning_rate, epochs)\n",
        "    print(f\"Trained weights (should be close to [{c}, {m}]):\", weights)\n",
        "\n",
        "    print(\"Step 2: Making predictions...\")\n",
        "    predictions = predict(X_with_bias, weights)\n",
        "    print(\"Sample Predictions:\", predictions[:5])\n",
        "\n",
        "    print(\"Step 3: Calculating Mean Squared Error...\")\n",
        "    mse = mean_squared_error(y, predictions)\n",
        "    print(\"Mean Squared Error (MSE):\", mse)\n",
        "\n",
        "    # Testing correctness\n",
        "    print(\"\\nTesting individual functions:\")\n",
        "    # Test MSE\n",
        "    test_y_true = np.array([2, 4, 6])\n",
        "    test_y_pred = np.array([2.1, 3.9, 5.8])\n",
        "    print(\"Test MSE (expected ~0.01):\", mean_squared_error(test_y_true, test_y_pred))\n",
        "\n",
        "    # Test gradient computation\n",
        "    test_X = np.array([[1, 1], [1, 2], [1, 3]])\n",
        "    test_y = np.array([1, 2, 3])\n",
        "    test_weights = np.array([0, 0])\n",
        "    print(\"Test Gradient (expected ~[-2, -6.67]):\", compute_gradient(test_X, test_y, test_weights))\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Logistic Regression Assignment with Bias Term\n",
        "\n",
        "Complete the following functions to implement logistic regression.\n",
        "Each function will be tested in the main function.\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def sigmoid(z):\n",
        "    \"\"\"\n",
        "    Compute the sigmoid function for the input z.\n",
        "\n",
        "    Parameters:\n",
        "    z (numpy array): Input values (linear combination of weights and features)\n",
        "\n",
        "    Returns:\n",
        "    numpy array: Sigmoid of input z\n",
        "    \"\"\"\n",
        "    # TODO: Implement sigmoid function\n",
        "    return 1/(1+np.exp(-z))\n",
        "\n",
        "\n",
        "def binary_cross_entropy(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Compute binary cross-entropy loss.\n",
        "\n",
        "    Parameters:\n",
        "    y_true (numpy array): True target values (0 or 1)\n",
        "    y_pred (numpy array): Predicted probabilities (0 to 1)\n",
        "\n",
        "    Returns:\n",
        "    float: Binary cross-entropy loss\n",
        "    \"\"\"\n",
        "    # TODO: Implement the formula for binary cross-entropy loss\n",
        "    m = y_true.shape[0]\n",
        "    l = -np.mean(y_true*np.log(y_pred) + (1-y_true)*np.log(1-y_pred))\n",
        "    return l\n",
        "\n",
        "\n",
        "def compute_gradient(X, y, weights):\n",
        "    \"\"\"\n",
        "    Compute the gradient for logistic regression.\n",
        "\n",
        "    Parameters:\n",
        "    X (numpy array): Feature matrix (n_samples x n_features)\n",
        "    y (numpy array): Target values (0 or 1)\n",
        "    weights (numpy array): Model weights\n",
        "\n",
        "    Returns:\n",
        "    numpy array: Gradient vector\n",
        "    \"\"\"\n",
        "    # TODO: Implement the gradient computation\n",
        "    m = X.shape[0]\n",
        "    predictions = sigmoid(X.dot(weights))\n",
        "    gradient = (1 / m) * X.T.dot(predictions - y)\n",
        "    return gradient\n",
        "\n",
        "\n",
        "def train_logistic_regression(X, y, learning_rate, epochs):\n",
        "    \"\"\"\n",
        "    Train a logistic regression model using gradient descent.\n",
        "\n",
        "    Parameters:\n",
        "    X (numpy array): Feature matrix (n_samples x n_features)\n",
        "    y (numpy array): Target values (0 or 1)\n",
        "    learning_rate (float): Learning rate for gradient descent\n",
        "    epochs (int): Number of iterations\n",
        "\n",
        "    Returns:\n",
        "    numpy array: Final weights\n",
        "    \"\"\"\n",
        "    # TODO: Initialize weights, iterate over epochs, and update weights\n",
        "    weights = np.zeros(X.shape[1])\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        gradient = compute_gradient(X, y, weights)\n",
        "        weights -= learning_rate * gradient\n",
        "\n",
        "    return weights\n",
        "\n",
        "\n",
        "def predict(X, weights, threshold=0.5):\n",
        "    \"\"\"\n",
        "    Predict the binary class labels (0 or 1) using the learned weights.\n",
        "\n",
        "    Parameters:\n",
        "    X (numpy array): Feature matrix (n_samples x n_features)\n",
        "    weights (numpy array): Model weights\n",
        "    threshold (float): Classification threshold (default 0.5)\n",
        "\n",
        "    Returns:\n",
        "    numpy array: Predicted class labels (0 or 1)\n",
        "    \"\"\"\n",
        "    # TODO: Implement the prediction logic\n",
        "    prob_values = sigmoid(X.dot(weights))\n",
        "    return (prob_values >= threshold).astype(int)\n",
        "\n",
        "\n",
        "def generate_data(num_samples, noise_level):\n",
        "    \"\"\"\n",
        "    Generate synthetic data for logistic regression (binary classification).\n",
        "\n",
        "    Parameters:\n",
        "    num_samples (int): Number of data points to generate\n",
        "    noise_level (float): Standard deviation of noise to add\n",
        "\n",
        "    Returns:\n",
        "    tuple: Feature matrix X and target vector y\n",
        "    \"\"\"\n",
        "    np.random.seed(42)  # For reproducibility\n",
        "    X = np.random.randn(num_samples, 2)  # Two features\n",
        "    true_weights = np.array([1.5, -2.0])  # True weights\n",
        "    bias = -0.5  # True bias\n",
        "    linear_combination = X @ true_weights + bias\n",
        "    probabilities = sigmoid(linear_combination + np.random.normal(0, noise_level, num_samples))\n",
        "    y = (probabilities >= 0.5).astype(int)\n",
        "    return X, y\n",
        "\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Main function to test all components of the assignment.\n",
        "    \"\"\"\n",
        "    # Generate synthetic data\n",
        "    num_samples = 100\n",
        "    noise_level = 0.1\n",
        "    X, y = generate_data(num_samples, noise_level)\n",
        "\n",
        "    # Append a column of ones to X for the bias term\n",
        "    X_with_bias = np.hstack((np.ones((X.shape[0], 1)), X))\n",
        "\n",
        "    # Parameters for training\n",
        "    learning_rate = 0.1\n",
        "    epochs = 500\n",
        "\n",
        "    print(\"Step 1: Training the logistic regression model...\")\n",
        "    weights = train_logistic_regression(X_with_bias, y, learning_rate, epochs)\n",
        "    print(\"Trained weights (including bias):\", weights)\n",
        "\n",
        "    print(\"Step 2: Making predictions...\")\n",
        "    predictions = predict(X_with_bias, weights)\n",
        "    print(\"Sample Predictions:\", predictions[:10])\n",
        "\n",
        "    print(\"Step 3: Calculating Binary Cross-Entropy Loss...\")\n",
        "    probabilities = sigmoid(X_with_bias @ weights)\n",
        "    loss = binary_cross_entropy(y, probabilities)\n",
        "    print(\"Binary Cross-Entropy Loss:\", loss)\n",
        "\n",
        "    # Testing correctness\n",
        "    print(\"\\nTesting individual functions:\")\n",
        "    # Test sigmoid\n",
        "    test_z = np.array([-2, 0, 2])\n",
        "    print(\"Test Sigmoid (expected ~[0.12, 0.5, 0.88]):\", sigmoid(test_z))\n",
        "\n",
        "    # Test binary cross-entropy\n",
        "    test_y_true = np.array([1, 0, 1])\n",
        "    test_y_pred = np.array([0.9, 0.1, 0.8])\n",
        "    print(\"Test BCE Loss (expected ~0.164):\", binary_cross_entropy(test_y_true, test_y_pred))\n",
        "\n",
        "    # Test gradient computation\n",
        "    test_X = np.array([[1, 0.5], [1, -0.5], [1, 1.0]])\n",
        "    test_y = np.array([1, 0, 1])\n",
        "    test_weights = np.array([0, 0])\n",
        "    print(\"Test Gradient (expected ~[-0.17, -0.33]):\", compute_gradient(test_X, test_y, test_weights))\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lTolOiRkYpRu",
        "outputId": "be381e63-7d6e-46a0-b88f-87a6a4474665"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1: Training the logistic regression model...\n",
            "Trained weights (including bias): [-0.75696574  2.40715567 -3.1005295 ]\n",
            "Step 2: Making predictions...\n",
            "Sample Predictions: [1 0 0 1 0 0 1 0 0 1]\n",
            "Step 3: Calculating Binary Cross-Entropy Loss...\n",
            "Binary Cross-Entropy Loss: 0.15233590209000375\n",
            "\n",
            "Testing individual functions:\n",
            "Test Sigmoid (expected ~[0.12, 0.5, 0.88]): [0.11920292 0.5        0.88079708]\n",
            "Test BCE Loss (expected ~0.164): 0.14462152754328741\n",
            "Test Gradient (expected ~[-0.17, -0.33]): [-0.16666667 -0.33333333]\n"
          ]
        }
      ]
    }
  ]
}